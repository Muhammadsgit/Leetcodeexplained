{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias-variance tradeoff is a fundamental concept in machine learning and statistics that deals with the problems of overfitting and underfitting.\n",
    "\n",
    "Bias: This refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. Bias measures how far off in general these models' predictions are from the correct value. A high-bias model is overly simplistic — it does not learn enough from the training data, leading to errors in both training and unseen data. This is known as underfitting.\n",
    "\n",
    "Variance: Variance refers to the model's sensitivity to the fluctuations in the training dataset. A model with high variance pays a lot of attention to training data and does not generalize well on the data it hasn’t seen before. Such models perform well on training data but have high error rates on test data. This is known as overfitting.\n",
    "\n",
    "The tradeoff:\n",
    "\n",
    "A model with high bias and low variance will oversimplify, resulting in systematic errors regardless of how much data we feed it.\n",
    "A model with high variance and low bias is overly complex, fitting the training data too closely and failing to generalize well to new data.\n",
    "The goal in most machine learning models is to find a good balance between bias and variance, minimizing the total error. This tradeoff is a key aspect of the model selection process — choosing not only the right algorithms but also the correct configuration (like the degree of a polynomial in regression, the depth of a decision tree, or the number of layers in a neural network) that provides the right level of model complexity.\n",
    "\n",
    "A common way to address the bias-variance tradeoff is through techniques such as cross-validation, where the model's performance is tested on multiple subsets of data to ensure it generalizes well, or regularization, which adds a penalty to more complex models to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1.b Live programing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "Level 1 - Programming\n",
    "You are a data scientist for a large retail organization. Currently your task involves mining for insights from Point-of_Sale systems which track item sales. You have data stored\n",
    "as follos in JSON files (for simplicity think that you have one file 'pos_data.json')\n",
    "Data Sample:\n",
    "[{'item': 'Stella Extra Strong', 'price': '$23.45'},\n",
    "{'item': 'Fosters Mild', 'price': '$12.99'},\n",
    "{'item': 'Heineken', 'price': '$29.45'},\n",
    "{'item': 'Stella Extra Strong', 'price': '$23.45'},\n",
    "{'item': 'Stella Extra Strong', 'price': '$23.45'},\n",
    "{'item': 'Fosters Mild', 'price': '$12.99'}]\n",
    "Your tasks are as follows.\n",
    "1. Write code to read in the JSON file to an appropriate python data structure (to solve Q2)\n",
    "2. Use base python libraries (not pandas) to get the following insights\n",
    " -> Item name with top total sales\n",
    " -> Item name with least total sales\n",
    " -> Item name with the most units sold\n",
    "Base Python libraries only (no 3rd party libraries e.g: numpy, pandas)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "f = open('pos_data.json')\n",
    "sales = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : practice opening different type of files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = [{'item': 'Stella Extra Strong', 'price': '$23.45'},\n",
    "{'item': 'Fosters Mild', 'price': '$12.99'},\n",
    "{'item': 'Heineken', 'price': '$29.45'},\n",
    "{'item': 'Stella Extra Strong', 'price': '$23.45'},\n",
    "{'item': 'Stella Extra Strong', 'price': '$23.45'},\n",
    "{'item': 'Fosters Mild', 'price': '$12.99'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write code to read in the JSON file to an appropriate python data structure (to solve Q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$23.45\n",
      "$12.99\n",
      "$29.45\n",
      "$23.45\n",
      "$23.45\n",
      "$12.99\n"
     ]
    }
   ],
   "source": [
    "#Converting price to float\n",
    "\n",
    "for item in sales:\n",
    "    print(item['price'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of all the items\n",
    "\n",
    "items_list = [item['item'] for item in sales]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a set of all the items\n",
    "items_set = [set(item['item']) for item in sales]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Natural Language Processing (NLP): Describe a project where you applied NLP. what challenges did you encounter in understanding and collecting human language, and how did you overcome them?\n",
    "\n",
    "2. Time Series Analysis: How would you forecast demand for a product? What models would you consider, and how would you account for seasonality and trends?\" \n",
    "\n",
    "3. A/B Testing: Design a test to evaluate two recommendation algorithms on an e-commerce site. How can you ensure the validity and reliability of the results?\n",
    "\n",
    "4. Ethics in Al: Discuss an Al ethical dilemma and how you would address potential biases in your models. \n",
    "\n",
    "5. Data Visualization: Give an example of a tool or technique you've used to simplify complex data insights to stakeholders who don't have a technical background.\n",
    "\n",
    "6. Machine Learning Algorithms: Describe the process of selecting the most appropriate machine learning algorithm for a given dataset. Discuss the trade-offs between bias and variance.\n",
    "\n",
    "7. Data Wrangling: Explain a complex data cleaning challenge you've faced and how you addressed it. What did you do to ensure data integrity and usability?\n",
    "\n",
    "8. Deep Learning: How would you implement a Convolutional Neural Network for image recognition tasks? \n",
    "\n",
    "9. Big Data Technologies: Discuss your experience with Hadoop and Spark. Explain the advantages and disadvantages of each in processing large datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
